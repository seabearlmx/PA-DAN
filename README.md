## PDA: Progressive Domain Adaptation for Semantic Segmentation
Muxin Liao, Shishun Tian, Yuhang Zhang, Guoguang Hua, Wenbin Zou, and Xia Li

Accepted by Knowledge-Based System

[Paper Link](https://www.sciencedirect.com/science/article/abs/pii/S0950705123009292).

## Abstract
The unsupervised domain adaptation semantic segmentation task is challenging due to the distribution shift problem between the source and the target domains. In this paper, we provide a novel perspective to address this problem. Currently, in the literature, it is shown that output-level domain adaptation networks (OL-DAN) can generate outputs with smaller distribution shift. Motivated by this phenomenon, a Progressive Domain Adaptation (PDA) framework is proposed, which uses the outputs generated by OL-DAN as the auxiliary domain images to progressively align the distribution between the source and target domains. Unlike existing two-stage input-level domain adaptation methods which use an image translation network as a standalone model to generate the auxiliary domain images, the PDA is an end-to-end framework that contains an OL-DAN and a domain fusion domain adaptation network (DF-DAN). The OL-DAN aims to gradually generate the outputs with smaller distribution shift and more accurate semantic structures as the auxiliary domain images in every iteration optimization. The DF-DAN is proposed to further mine the domain-invariant information from the auxiliary images and then fuse the features learned from the original images and the auxiliary images to obtain a richer representation. Finally, the distribution between the source and target domains is aligned to optimize the OL-DAN and DF-DAN. Experiments demonstrate that the proposed PDA achieves superior performance on three cross-domain semantic segmentation benchmarks.

## Preparation

### Pre-requisites
* Python 3.7
* Pytorch >= 1.1.0
* CUDA 9.0 or higher

### Installation
0. Clone the repo:
```bash
$ git clone https://github.com/seabearlmx/PDA
$ cd PDA
```

### Datasets
By default, the datasets are put in ```<root_dir>/DADatasets```. 

* **GTA5**: Please follow the instructions [here](https://download.visinf.tu-darmstadt.de/data/from_games/) to download images and semantic segmentation annotations. The GTA5 dataset directory should have this basic structure:
```bash
<root_dir>/DADatasets/GTA5/                               % GTA dataset root
<root_dir>/DADatasets/GTA5/images/                        % GTA images
<root_dir>/DADatasets/GTA5/labels/                        % Semantic segmentation labels
...
```

* **Cityscapes**: Please follow the instructions in [Cityscape](https://www.cityscapes-dataset.com/) to download the images and validation ground-truths. The Cityscapes dataset directory should have this basic structure:
```bash
<root_dir>/DADatasets/Cityscapes/                         % Cityscapes dataset root
<root_dir>/DADatasets/Cityscapes/leftImg8bit              % Cityscapes images
<root_dir>/DADatasets/Cityscapes/leftImg8bit/val
<root_dir>/DADatasets/Cityscapes/gtFine                   % Semantic segmentation labels
<root_dir>/DADatasets/Cityscapes/gtFine/val
...
```

For evaluation, execute:
```bash
$ cd <root_dir>/PDA
$ python test.py --cfg ./configs/padan.yml
```

### Training
For the experiments done in the paper, we used pytorch 1.1.0 and CUDA 9.0. To ensure reproduction, the random seed has been fixed in the code. Still, you may need to train a few times to reach the comparable performance.

By default, logs and snapshots are stored in ```<root_dir>/experiments``` with this structure:
```bash
<root_dir>/experiments/logs
<root_dir>/experiments/snapshots
```

To train PDA:
```bash
$ cd <root_dir>/PDA
$ python train.py --cfg ./configs/padan.yml

```

### Testing
To test PDA:
```bash
$ cd <root_dir>/PDA
$ python test.py --cfg ./configs/padan.yml
```

## Acknowledgements
This codebase is heavily borrowed from [AdaptSegNet](https://github.com/wasidennis/AdaptSegNet) and [AdvEnt](https://github.com/valeoai/ADVENT).

## License
PDA is released under the [MIT license](./LICENSE).

## Update status
The code (V1) is uploaded. (2021-07-19)
