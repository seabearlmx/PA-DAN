## PDA: Progressive Domain Adaptation for Semantic Segmentation
Muxin Liao, Shishun Tian, Yuhang Zhang, Guoguang Hua, Wenbin Zou, and Xia Li

## Abstract
The unsupervised domain adaptation (UDA) semantic segmentation task is challenging due to the domain shift problem between the source and the target domains. In this paper, we provide a novel perspective to address this problem. It is shown in current literature that the output-level alignment strategy can generate outputs with a smaller domain gap. Motivated by this phenomenon, in this paper, we propose the Progressive Domain Adaptation (PDA) which uses the outputs generated by output-level domain adaptation networks (OL-DAN) as the auxiliary domain images to progressively align the distribution between the source and target domains. Unlike most existing two-stage input-level domain adaptation methods which separately use image translation networks to generate the auxiliary domain images, the PDA is an end-to-end framework that contains an OL-DAN and a domain fusion domain adaptation network (DF-DAN). The OL-DAN aims to gradually generate the outputs with a smaller domain gap and more accurate semantic structures as the auxiliary domain images in every iteration optimization. The DF-DAN is proposed to further mine the domain-invariant information from the auxiliary images and then fuse the features learned from the original domain and the auxiliary domain for obtaining a richer representation. Finally, the output-level alignment strategy is used to align the distribution of the source and target domains for optimizing the OL-DAN and DF-DAN simultaneously. Extensive experiments demonstrate the effectiveness of the proposed PDA on two challenging cross-domain semantic segmentation datasets.

## Preparation

### Pre-requisites
* Python 3.7
* Pytorch >= 1.1.0
* CUDA 9.0 or higher

### Installation
0. Clone the repo:
```bash
$ git clone https://github.com/seabearlmx/PDA
$ cd PA-DAN
```

### Datasets
By default, the datasets are put in ```<root_dir>/DADatasets```. 

* **GTA5**: Please follow the instructions [here](https://download.visinf.tu-darmstadt.de/data/from_games/) to download images and semantic segmentation annotations. The GTA5 dataset directory should have this basic structure:
```bash
<root_dir>/DADatasets/GTA5/                               % GTA dataset root
<root_dir>/DADatasets/GTA5/images/                        % GTA images
<root_dir>/DADatasets/GTA5/labels/                        % Semantic segmentation labels
...
```

* **Cityscapes**: Please follow the instructions in [Cityscape](https://www.cityscapes-dataset.com/) to download the images and validation ground-truths. The Cityscapes dataset directory should have this basic structure:
```bash
<root_dir>/DADatasets/Cityscapes/                         % Cityscapes dataset root
<root_dir>/DADatasets/Cityscapes/leftImg8bit              % Cityscapes images
<root_dir>/DADatasets/Cityscapes/leftImg8bit/val
<root_dir>/DADatasets/Cityscapes/gtFine                   % Semantic segmentation labels
<root_dir>/DADatasets/Cityscapes/gtFine/val
...
```

### Pre-trained models
Pre-trained models can be downloaded [here](https://github.com/seabearlmx/PDA/releases) and put in ```<root_dir>/padan/pretrained_models```

## Running the code
Please follow the [here](https://github.com/seabearlmx/PDA/releases) to download model.

For evaluation, execute:
```bash
$ cd <root_dir>/padan
$ python test.py --cfg ./configs/padan.yml
```

### Training
For the experiments done in the paper, we used pytorch 1.1.0 and CUDA 9.0. To ensure reproduction, the random seed has been fixed in the code. Still, you may need to train a few times to reach the comparable performance.

By default, logs and snapshots are stored in ```<root_dir>/experiments``` with this structure:
```bash
<root_dir>/experiments/logs
<root_dir>/experiments/snapshots
```

To train PA-DAN:
```bash
$ cd <root_dir>/padan
$ python train.py --cfg ./configs/padan.yml

```

### Testing
To test PDA:
```bash
$ cd <root_dir>/padan
$ python test.py --cfg ./configs/padan.yml
```

## Acknowledgements
This codebase is heavily borrowed from [AdaptSegNet](https://github.com/wasidennis/AdaptSegNet) and [AdvEnt](https://github.com/valeoai/ADVENT).

## License
PDA is released under the [MIT license](./LICENSE).

## Update status
The code (V1) is uploaded. (2021-07-19)
